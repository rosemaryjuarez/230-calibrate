---
title: "Calibrate Sagehen streamflow"
authors: "Rosemary Juarez, Vanessa Salgado, Liane Chen"
date: "2024-04-30"
output: html_document
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(sensitivity)
library(tidyverse)
library(purrr)
library(ggpubr)
library(here)
```

```{r, read in data}

#--------------------------
#   set working directory
#--------------------------
setwd(here::here())


#--------------------------
#   read in data
#--------------------------
#reading in observed stream flow data
sager = read.table(here("data","sager.txt"), header=T)

#simulated data from multiple model runs
# multiple results - assuming we've run the model for multiple years, each column
# is streamflow for a different parameter set
msage = read.table(here("data","sagerm.txt"), header=T)


#--------------------------
#   preprocessing data
#--------------------------

# add date ton sager
sager = sager %>%
  mutate(date = paste(day,month,year, sep="/"))

#converting to datetime format
sager$date = as.Date(sager$date,"%d/%m/%Y")

# "model" and "obs" columns are combined into a single column, and their corresponding values are placed in another column. 
sagerl = sager %>% pivot_longer(cols=c("model","obs"), names_to="source",
                                  values_to="flow")


#--------------------------
#   sourcing in R scripts
#--------------------------

#Nash-Sutcliffe Efficiency
source("R/nse.R")

#relative error between the model and observations
source("R/relerr.R")

#combined performance metric between NSE and relative error
source("R/cper.R")


#calculates performance metrics focusing on high flow periods
# computes the error and correlation of annual max flows and monthly flows during 
# April(high flow months)
source("R/compute_highflowmetrics.R")

#computes a combined metric giving different weights to each metrics
source("R/compute_highflowmetrics_all.R")

```

## Assignment Instructions

Part 1: Come up with a combined metric that you think is interesting

-   if you can, try to include at least one metric (as part of your combined metric) that needs to be transformed
-   be creative - you can subset, aggregate, focus only on particular type of years or days
-   think about ecological or human water uses that depend on certain flow conditions

Part II

-   Perform a split-sample calibration on the Sagehen model output (sagerm.txt)
    -   you can decide what years to pick for pre and post calibration
    -   use your performance metric from Part I
-   Find the best and worst parameter set, given your performance metric
-   Graph something about streamflow (e.g daily, mean August, or ?) for the best parameter set
-   Compute and plot how the performance of the model using the best parameter set changed in pre and post calibration periods (that you chose)
-   Add the 'best' parameter set column number number to the quiz linked below (so we can compare how different metrics influence which parameter you pick)
-   Write 2-3 sentences to explain your metric design and comment on model performance based on your metric

Use to Best Parameter (Column) to record your 'best' parameter sets ([GO TO CANVAS FOR THIS TEST](https://ucsb.instructure.com/courses/19594/assignments/240440))

Hand in Rmarkdown and R file with your performance metric

# creating our model metric

For our assignment, we want to observe the time series of **high flow periods** and **peak flow periods** within the "blank river" as noted in the graph below, we observed that during the year 1977, peak values of high water periods was during the month of April and May.

```{r, model observations}
ggplot(subset(sagerl, wy==1977), aes(date, flow, col=source,
linetype=source))+
  geom_line()+
  scale_y_continuous(trans="log")+
  labs(y="streamflow
mm/day")
```

Because we want to evaluate the performance of the model based on high stream flow conditions in April, we will use three metrics to check this out:

-   `NSE.R` (Nash-Sutcliffe Efficiency)

-    `relerr.R`(Relative Error)

-    `cper.R` (relative error between model and obs)

```{r}
source(here("R","cper.R")) 
source(here("R","relerr.R"))
source(here("R","cper.R")) 

nse
relerr
cper
```

```{r}
nse(m=sager$model, o=sager$obs)
```

```{r}
relerr(m=sager$model, o=sager$obs)*100
```

```{r}
cper(m=sager$model, o=sager$obs, weight.nse=0.8)
```

```{r}
sager_mos = sager %>% group_by(month) %>% summarize(model=sum(model), obs=sum(obs))
nse_result <- nse(sager_mos$model, sager_mos$obs)
nse_result
```

```{r}
cper_result <- cper(m=sager_mos$model, o=sager_mos$obs, weight.nse=0.8)
cper_result
```

```{r}
# just look at april flow
# first sum by month
tmp = sager %>% group_by(wy, month) %>% summarize(model=sum(model), obs=sum(obs))

# now extract april
sager_apr = subset(tmp, month==4)
sager_result <- cor(sager_apr$model, sager_apr$obs)
sager_result

```

```{r}
# now extract may
sager_may = subset(tmp, month==5)
cor(sager_may$model, sager_may$obs)
```

##Part 1: Combined Metric
```{r}
# Define your combined metric function
combined_metric <- function(nse, sager, cper, weights = c(0.3, 0.3, 0.4)) {
  # Transform if necessary
  transformed_nse <- nse # No transformation for NSE
  transformed_sager <- log(1 + abs(sager)) # Transforming relative error
  transformed_cper <- cper # No transformation for cper
  
  # Combine metrics with weights
  combined <- (weights[1] * transformed_nse) + (weights[2] * transformed_sager) + (weights[3] * transformed_cper)
  
  return(combined)
}

# Calculate combined metric
combined_result <- combined_metric(nse_result, sager_result, cper_result)

# Print combined metric result
print(combined_result)

```

## Part 2: Split Sample Calibration
```{r}
# Split-sample calibration function
split_sample_calibration <- function(msage, pre_calibration_years, post_calibration_years, metric_weights) {
  # Filter data for pre and post calibration periods
  pre_calibration_data <- filter(data, year %in% pre_calibration_years)
  post_calibration_data <- filter(data, year %in% post_calibration_years)
  
  # Calibrate model parameters using pre-calibration data
  
  # Evaluate model performance using your combined metric
  pre_calibration_performance <- combined_metric(nse(pre_calibration_data), relerr(pre_calibration_data), cper(pre_calibration_data), metric_weights)
  
  # Identify the best parameter set from pre-calibration
  
  # Calibrate model parameters using post-calibration data
  
  # Evaluate model performance using your combined metric
  post_calibration_performance <- combined_metric(nse(post_calibration_data), relerr(post_calibration_data), cper(post_calibration_data), metric_weights)
  
  # Plot how the performance of the model changed pre and post calibration
  plot_calibration_performance(pre_calibration_performance, post_calibration_performance)
  
  # Return the best parameter set
  return(best_parameter_set)
}

# Call the split-sample calibration function with appropriate arguments
best_parameter_set <- split_sample_calibration(msage, pre_calibration_years, post_calibration_years, weights)

```


